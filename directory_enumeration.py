import os
import sys
import requests
import concurrent.futures
import csv
from fake_useragent import UserAgent  # library to generate random User-Agent strings


# randomly select a user agent
def get_random_user_agent():
    ua = UserAgent()
    return ua.random


# Function to perform directory enumeration with user-agent rotation
def enumerate_directory(directory_url, method):
    try:
        # Get a random user agent for each request
        user_agent = get_random_user_agent()
        headers = {'User-Agent': user_agent}

        # requests session object to apply max_redirects globally
        with requests.Session() as session:
            session.max_redirects = 10
            # Make request using specified HTTP method
            if method == "GET":
                response = session.get(directory_url, headers=headers, allow_redirects=True)
            elif method == "HEAD":
                response = session.head(directory_url, headers=headers, allow_redirects=True)
            elif method == "POST":
                response = session.post(directory_url, headers=headers, allow_redirects=True)
            elif method == "PUT":
                response = session.put(directory_url, headers=headers, allow_redirects=True)
            elif method == "DELETE":
                response = session.delete(directory_url, headers=headers, allow_redirects=True)

            if response.status_code == 200:
                return directory_url
    except requests.exceptions.TooManyRedirects:
        print(f"Too many redirects for {directory_url}")
    except requests.exceptions.RequestException as e:
        print(f"Error accessing {directory_url}: {e}")


# Function to read the wordlist file and construct directory URLs
def main():
    # Check if the correct number of arguments are provided
    if len(sys.argv) != 5:
        print("Usage: python3 directory_enumeration.py <base_url> <path_to_wordlist> <method> <output_file>")
        return

    # Base URL for the target website
    base_url = sys.argv[1]

    # check that URL includes http:// or https://
    if not base_url.startswith("http://") and not base_url.startswith("https://"):
        print("Error: URL must include http:// or https://")
        return

    # path to wordlist file
    wordlist_file = sys.argv[2]

    # Check wordlist exists
    if not os.path.isfile(wordlist_file):
        print("Error: Wordlist file not found.")
        return

    # HTTP method
    method = sys.argv[3].upper()

    # Output file
    output_file = sys.argv[4]

    # Read the wordlist file
    with open(wordlist_file) as file:
        directory_list = file.read().splitlines()

    # Construct directory URLs
    directory_urls = [f"{base_url.rstrip('/')}/{directory}/" for directory in directory_list]

    # Directory enumeration using threading
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(lambda url: enumerate_directory(url, method), directory_urls)

    # Write valid directories to CSV file
    valid_directories = [result for result in results if result is not None]
    with open(output_file, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Valid Directories'])
        writer.writerows([[directory] for directory in valid_directories])

    print(f"Results saved in {output_file}")


if __name__ == "__main__":
    main()

